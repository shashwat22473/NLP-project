{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11150223,"sourceType":"datasetVersion","datasetId":6956467}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install indic-nlp-library\n!pip install stopwordsiso","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:09:06.776544Z","iopub.execute_input":"2025-03-24T22:09:06.776828Z","iopub.status.idle":"2025-03-24T22:09:16.169061Z","shell.execute_reply.started":"2025-03-24T22:09:06.776806Z","shell.execute_reply":"2025-03-24T22:09:16.167994Z"}},"outputs":[{"name":"stdout","text":"Collecting indic-nlp-library\n  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\nCollecting sphinx-argparse (from indic-nlp-library)\n  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (0.2.4)\nCollecting morfessor (from indic-nlp-library)\n  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->indic-nlp-library) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2025.1)\nRequirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (8.1.3)\nRequirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\nRequirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\nRequirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\nRequirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\nRequirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\nRequirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\nRequirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\nRequirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.4)\nRequirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.1)\nRequirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\nRequirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.16.0)\nRequirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\nRequirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\nRequirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\nRequirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.2)\nRequirement already satisfied: tomli>=2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->indic-nlp-library) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->indic-nlp-library) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->indic-nlp-library) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->indic-nlp-library) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->indic-nlp-library) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.1.31)\nDownloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\nDownloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\nInstalling collected packages: morfessor, sphinx-argparse, indic-nlp-library\nSuccessfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.5.2\nCollecting stopwordsiso\n  Downloading stopwordsiso-0.6.1-py3-none-any.whl.metadata (2.5 kB)\nDownloading stopwordsiso-0.6.1-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: stopwordsiso\nSuccessfully installed stopwordsiso-0.6.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport emoji\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom indicnlp.tokenize import indic_tokenize\nimport stopwordsiso\nfrom torch.utils.data import Dataset, DataLoader\nimport gensim\nfrom sklearn.metrics import f1_score\nimport torch.optim as optim\nfrom tqdm import tqdm\n\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"stopwords\", quiet=True)\nnltk.download(\"wordnet\", quiet=True)\nnltk.download(\"omw-1.4\", quiet=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:09:16.170345Z","iopub.execute_input":"2025-03-24T22:09:16.170608Z","iopub.status.idle":"2025-03-24T22:09:39.536897Z","shell.execute_reply.started":"2025-03-24T22:09:16.170571Z","shell.execute_reply":"2025-03-24T22:09:39.536133Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# i don't know why but this makes the wordnet error disappear\n\nfrom spacy import load\nnltk.download('omw-1.4')\nnltk.download('wordnet')\nnltk.download('wordnet2022')\nnlp = load('en_core_web_sm')\n\n! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet # temp fix for lookup error.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:09:39.538827Z","iopub.execute_input":"2025-03-24T22:09:39.539358Z","iopub.status.idle":"2025-03-24T22:09:44.192649Z","shell.execute_reply.started":"2025-03-24T22:09:39.539334Z","shell.execute_reply":"2025-03-24T22:09:44.191506Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package wordnet2022 to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet2022.zip.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# just loading all the training and testing dataset\n\ndataset_train_english1=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/training/train_en_l1.csv\")\ndataset_train_english2=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/training/train_en_l2.csv\")\ndataset_train_english3=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/training/train_en_l3.csv\")\ndataset_train_english=pd.concat([dataset_train_english1,dataset_train_english2,dataset_train_english3],ignore_index=True)\n\ndataset_train_hindi1=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/training/train_hi_l1.csv\")\ndataset_train_hindi2=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/training/train_hi_l2.csv\")\ndataset_train_hindi3=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/training/train_hi_l3.csv\")\ndataset_train_hindi=pd.concat([dataset_train_hindi1,dataset_train_hindi2,dataset_train_hindi3],ignore_index=True)\n\ndataset_train_tamil1=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/training/train_ta_l1.csv\")\ndataset_train_tamil2=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/training/train_ta_l2.csv\")\ndataset_train_tamil3=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/training/train_ta_l3.csv\")\ndataset_train_tamil=pd.concat([dataset_train_tamil1,dataset_train_tamil2,dataset_train_tamil3],ignore_index=True)\n\ndataset_test_english1=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/testing/test_en_l1.csv\")\ndataset_test_english2=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/testing/test_en_l2.csv\")\ndataset_test_english3=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/testing/test_en_l3.csv\")\ndataset_test_english=pd.concat([dataset_test_english1,dataset_test_english2,dataset_test_english3],ignore_index=True)\n\n\ndataset_test_hindi1=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/testing/test_hi_l1.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\",engine='python')\ndataset_test_hindi2=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/testing/test_hi_l2.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\",engine='python')\ndataset_test_hindi3=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/testing/test_hi_l3.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\",engine='python')\ndataset_test_hindi=pd.concat([dataset_test_hindi1, dataset_test_hindi2, dataset_test_hindi3], ignore_index=True)\n\ndataset_test_tamil1=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/testing/test_ta_l1.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\ndataset_test_tamil2=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/testing/test_ta_l2.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\ndataset_test_tamil3=pd.read_csv(\"/kaggle/input/nlp-project-dataset/uli_dataset-main/testing/test_ta_l3.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\ndataset_test_tamil=pd.concat([dataset_test_tamil1,dataset_test_tamil2,dataset_test_tamil3],ignore_index=True)\n\nprint(dataset_train_english.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:09:44.194415Z","iopub.execute_input":"2025-03-24T22:09:44.195622Z","iopub.status.idle":"2025-03-24T22:09:44.962167Z","shell.execute_reply.started":"2025-03-24T22:09:44.195592Z","shell.execute_reply":"2025-03-24T22:09:44.961169Z"}},"outputs":[{"name":"stdout","text":"Index(['text', 'key', 'en_a1', 'en_a2', 'en_a3', 'en_a4', 'en_a5', 'en_a6'], dtype='object')\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def convert_emojis(text):\n    return emoji.demojize(text, delimiters=(\" \", \" \"))  \n\ndef remove_mentions(text):\n    return re.sub(r\"@\\w+\", \"<USER>\", text)\n\ndef reduce_repetitions(text):\n    return re.sub(r'(.)\\1+', r'\\1\\1', text) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:09:44.963417Z","iopub.execute_input":"2025-03-24T22:09:44.963781Z","iopub.status.idle":"2025-03-24T22:09:44.968588Z","shell.execute_reply.started":"2025-03-24T22:09:44.963748Z","shell.execute_reply":"2025-03-24T22:09:44.967477Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\"\"\" if number of annotator  who classify a  tweet as abuse is greater than number of annotator who do not  or numeber of annotator who classify as\n\n    abuse is greater than equal to 2 , then we classify the tweet as abuse .\"\"\"  \n\ndef preprocess_dataset(dataset,language):\n    \n    dataset.columns=dataset.columns.str.strip()\n    \n    dataset=dataset.drop(columns=[\"key\"])\n    dataset.dropna(subset=[\"text\"], inplace=True)\n    abuse=[]\n\n    # classifying the tweet as abuse or not\n    \n    for index,record in dataset.iterrows():\n        count_0=0\n        count_1=0\n        for keys in record.keys():\n            if (keys!='text'):\n                if (record[keys]==1):count_1+=1\n                if (record[keys]==0):count_0+=1\n        \n        if (count_1>=count_0  or count_1>1 ):abuse.append(1)\n        else:abuse.append(0)\n    # keeping only relevant columns in the dataset\n    \n    dataset['abuse']=abuse\n    dataset=dataset[['text','abuse']]\n\n    # preprocessing the text\n\n    dataset['text']=dataset['text'].apply(remove_mentions) # remove user's name with a tag <USER>\n    dataset['text']= dataset['text'].apply(convert_emojis) # convert emojies into text description\n    dataset[\"text\"]=dataset[\"text\"].str.replace(r\"http\\S+|www\\S+\", \"<URL>\", regex=True) # removes any urls with tag <URL>\n    dataset[\"text\"]=dataset[\"text\"].str.replace(r\"[^A-Za-z0-9\\s]\", \"\", regex=True).str.strip() #removes any special character and empty spaces\n\n    if (language=='en'):\n        dataset[\"text\"] = dataset[\"text\"].apply(nltk.word_tokenize)\n        stop_words = set(stopwords.words(\"english\"))\n        dataset[\"text\"] = dataset[\"text\"].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n        \n        lemmatizer = WordNetLemmatizer()\n        dataset[\"text\"] = dataset[\"text\"].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n        dataset[\"text\"] = dataset[\"text\"].apply(lambda x: \" \".join(x))\n        dataset['text'] = dataset['text'].apply(reduce_repetitions)\n    else:\n        \n        dataset[\"text\"] = dataset[\"text\"].apply(lambda x: indic_tokenize.trivial_tokenize(x, lang=language))\n        language_stopwords=stopwordsiso.stopwords(language)\n        dataset[\"text\"] = dataset[\"text\"].apply(lambda x: [word for word in x if word not in language_stopwords])\n        dataset[\"text\"] = dataset[\"text\"].apply(lambda x: \" \".join(x))\n\n    return dataset\n        \n    \n\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:09:44.969843Z","iopub.execute_input":"2025-03-24T22:09:44.970230Z","iopub.status.idle":"2025-03-24T22:09:44.991939Z","shell.execute_reply.started":"2025-03-24T22:09:44.970189Z","shell.execute_reply":"2025-03-24T22:09:44.990950Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\ndataset_train_english=preprocess_dataset(dataset_train_english,language='en')\ndataset_test_english=preprocess_dataset(dataset_test_english,language='en')\nprint(\"english dataset preprocessed\")\ndataset_train_hindi=preprocess_dataset(dataset_train_hindi,language='hi')\ndataset_test_hindi=preprocess_dataset(dataset_test_hindi,language='hi')\nprint(\"hindi dataset preprocessed\")\ndataset_train_tamil=preprocess_dataset(dataset_train_tamil,language='ta')\ndataset_test_tamil=preprocess_dataset(dataset_test_tamil,language='ta')\nprint('tamil dataset preprocessed')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:09:44.993114Z","iopub.execute_input":"2025-03-24T22:09:44.993487Z","iopub.status.idle":"2025-03-24T22:10:06.750804Z","shell.execute_reply.started":"2025-03-24T22:09:44.993452Z","shell.execute_reply":"2025-03-24T22:10:06.750058Z"}},"outputs":[{"name":"stdout","text":"english dataset preprocessed\nhindi dataset preprocessed\ntamil dataset preprocessed\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\n\nMODEL_NAME = \"bert-base-multilingual-cased\"\ntokenizer = BertTokenizer.from_pretrained(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:10:06.752405Z","iopub.execute_input":"2025-03-24T22:10:06.752615Z","iopub.status.idle":"2025-03-24T22:10:23.652179Z","shell.execute_reply.started":"2025-03-24T22:10:06.752596Z","shell.execute_reply":"2025-03-24T22:10:23.651371Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cc52c2c18ca487dbf5091a5d2c5867e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3357b7132c64dac9229a8aedc6a0a40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eea38af4132d4afa82d958028d7fafe6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51e835416ba747659010190d031e7e02"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"class AbuseDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        encoded = self.tokenizer(\n            self.texts[idx], \n            padding=\"max_length\", \n            truncation=True, \n            max_length=self.max_length, \n            return_tensors=\"pt\"\n        )\n        input_ids = encoded[\"input_ids\"].squeeze()\n        attention_mask = encoded[\"attention_mask\"].squeeze()\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        return input_ids, attention_mask, label\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T22:10:23.653131Z","iopub.execute_input":"2025-03-24T22:10:23.653690Z","iopub.status.idle":"2025-03-24T22:10:23.659710Z","shell.execute_reply.started":"2025-03-24T22:10:23.653651Z","shell.execute_reply":"2025-03-24T22:10:23.658925Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train(training_dataset, testing_dataset, Model_name):\n    model = BertForSequenceClassification.from_pretrained(Model_name, num_labels=2)  \n    tokenizer = BertTokenizer.from_pretrained(Model_name)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    train_dataset = AbuseDataset(training_dataset[\"text\"].tolist(), training_dataset[\"abuse\"].tolist(), tokenizer)\n    test_dataset = AbuseDataset(testing_dataset[\"text\"].tolist(), testing_dataset[\"abuse\"].tolist(), tokenizer)\n    \n    BATCH_SIZE = 8\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n    \n    EPOCHS = 0\n    model.train()\n    \n    for epoch in range(EPOCHS):\n        total_loss = 0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=True)  \n        \n        for batch in progress_bar:  \n            input_ids, attention_mask, labels = batch\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask).logits\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            progress_bar.set_postfix(loss=loss.item()) \n        \n        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n\n    # Evaluation\n    model.eval()\n    y_preds = []\n    y_true = []\n    \n    with torch.no_grad():\n        progress_bar = tqdm(test_loader, desc=\"Evaluating\", leave=True)  \n\n        for batch in progress_bar:  \n            input_ids, attention_mask, labels = batch\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask).logits\n            preds = torch.argmax(outputs, dim=1)\n            y_preds.extend(preds.cpu().numpy())\n            y_true.extend(labels.cpu().numpy())\n\n        progress_bar.close() \n    \n    macro_f1 = f1_score(y_true, y_preds, average=\"macro\")\n    print(f\"Macro F1-score: {macro_f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:59:27.755281Z","iopub.execute_input":"2025-03-24T23:59:27.755599Z","iopub.status.idle":"2025-03-24T23:59:27.764550Z","shell.execute_reply.started":"2025-03-24T23:59:27.755567Z","shell.execute_reply":"2025-03-24T23:59:27.763741Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()  \ngc.collect()  \nprint(\"*\"*20)\nprint('Result for English Language')\ntrain(dataset_train_english,dataset_test_english,MODEL_NAME)\nprint()\n\nprint(\"*\"*20)\nprint('Result for Hindi Language')\ntrain(dataset_train_hindi,dataset_test_hindi,MODEL_NAME)\nprint()\n\nprint(\"*\"*20)\nprint('Result for Tamil Language')\ntrain(dataset_train_tamil,dataset_test_tamil,MODEL_NAME)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:59:30.550339Z","iopub.execute_input":"2025-03-24T23:59:30.550611Z","iopub.status.idle":"2025-03-25T00:01:07.107652Z","shell.execute_reply.started":"2025-03-24T23:59:30.550590Z","shell.execute_reply":"2025-03-25T00:01:07.106768Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"********************\nResult for English Language\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 416/416 [00:27<00:00, 14.87it/s]\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Macro F1-score: 0.5083\n\n********************\nResult for Hindi Language\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 569/569 [00:38<00:00, 14.91it/s]\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Macro F1-score: 0.5063\n\n********************\nResult for Tamil Language\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 426/426 [00:27<00:00, 15.44it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1-score: 0.3669\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}